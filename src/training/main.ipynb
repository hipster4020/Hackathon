{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1k5qvqSzGjHbIQ6cuNMR6Xk8VVyIA7DO8","authorship_tag":"ABX9TyOAwNRNeouIIHyGAQ6s2N1T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["# Install"],"metadata":{"id":"JyMQA9yPsofV"}},{"cell_type":"code","source":["!pip3 install transformers"],"metadata":{"id":"J7jVffUnsrip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install datasets"],"metadata":{"id":"pGw9C_koyAbV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install hydra-core"],"metadata":{"id":"wtNA6LR83gm0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install wandb"],"metadata":{"id":"ywPNiOhKYRfK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install accelerate"],"metadata":{"id":"LQGQSWfAY6pz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Ss8mh3XdzUyI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671270501702,"user_tz":-540,"elapsed":10244,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}},"outputId":"3f049d74-11f4-4d7d-ec42-a1a0de06b0a6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip3 list | grep datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nIc2ivUf_kg_","executionInfo":{"status":"ok","timestamp":1671178542305,"user_tz":-540,"elapsed":840,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}},"outputId":"df335c36-7ae2-4b53-86f5-bf1e5a2a9b7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["datasets                      2.7.1\n","tensorflow-datasets           4.6.0\n","vega-datasets                 0.9.0\n"]}]},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xqvd6gpraRFV","executionInfo":{"status":"ok","timestamp":1671270506454,"user_tz":-540,"elapsed":9,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}},"outputId":"b7cd53bc-59dd-4c7f-f283-3dab642139ad"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Not connected to a GPU\n"]}]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S-ETFYQfaRUK","executionInfo":{"status":"ok","timestamp":1671270508070,"user_tz":-540,"elapsed":4,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}},"outputId":"4110b496-fa1d-4839-9922-4ef5e94b523d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 37.8 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}]},{"cell_type":"markdown","source":["# run"],"metadata":{"id":"wIw3DXFesrUy"}},{"cell_type":"markdown","source":["load"],"metadata":{"id":"aX4NlYL9zO7Z"}},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/Hackathon/src/training')\n","\n","!python3 train.py"],"metadata":{"id":"U2r1uXozsng8","executionInfo":{"status":"ok","timestamp":1671271033869,"user_tz":-540,"elapsed":522514,"user":{"displayName":"ë°•ì„±í™˜","userId":"16608640204304641951"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ad2b127d-24f9-4240-ae1f-0676fa00b767"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: 100% 252/252 [00:00<00:00, 230kB/s]\n","Downloading: 100% 2.51M/2.51M [00:00<00:00, 12.1MB/s]\n","Downloading: 100% 88.0/88.0 [00:00<00:00, 74.3kB/s]\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-e83bf839bf6ac51f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n","Downloading data files: 100% 2/2 [00:00<00:00, 2729.78it/s]\n","\n","Extracting data files #1: 100% 1/1 [00:00<00:00, 96.73obj/s]\n","Extracting data files #0: 100% 1/1 [00:00<00:00, 37.75obj/s]\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-e83bf839bf6ac51f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 94.55it/s]\n","100% 22074/22074 [00:22<00:00, 972.51ba/s]\n","100% 700/700 [00:00<00:00, 909.73ba/s]\n","100% 453/453 [03:56<00:00,  1.91ba/s]\n","100% 16/16 [00:10<00:00,  1.52ba/s]\n","Downloading: 100% 839/839 [00:00<00:00, 577kB/s]\n","Downloading: 100% 12.3G/12.3G [02:16<00:00, 90.1MB/s]\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.7\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Hackathon/src/training/wandb/run-20221217_095657-2gqbmixp\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhackathon\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/psh_pat/hackathon\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/psh_pat/hackathon/runs/2gqbmixp\u001b[0m\n","  0% 0/70590 [00:00<?, ?it/s]Error executing job with overrides: []\n","Traceback (most recent call last):\n","  File \"train.py\", line 52, in main\n","    trainer.train()\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1527, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1775, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2523, in training_step\n","    loss = self.compute_loss(model, inputs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2555, in compute_loss\n","    outputs = model(**inputs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/gptj/modeling_gptj.py\", line 821, in forward\n","    transformer_outputs = self.transformer(\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/gptj/modeling_gptj.py\", line 630, in forward\n","    hidden_states = self.drop(hidden_states)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/dropout.py\", line 59, in forward\n","    return F.dropout(input, self.p, self.training, self.inplace)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\", line 1252, in dropout\n","    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n","RuntimeError: \"bernoulli_scalar_cpu_\" not implemented for 'Half'\n","\n","Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mhackathon\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/psh_pat/hackathon/runs/2gqbmixp\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221217_095657-2gqbmixp/logs\u001b[0m\n","  0% 0/70590 [00:04<?, ?it/s]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"FkLDGbLxAzLk"},"execution_count":null,"outputs":[]}]}