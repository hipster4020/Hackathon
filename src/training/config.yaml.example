PATH:
    tokenizer_config: 
        pretrained_model_name_or_path: kakaobrain/kogpt
        revision: KoGPT6B-ryan1.5b-float16  # or float32 version: revision=KoGPT6B-ryan1.5b
        mask_token: "[MASK]"

    model_config:
        pretrained_model_name_or_path: kakaobrain/kogpt
        revision: KoGPT6B-ryan1.5b-float16  # or float32 version: revision=KoGPT6B-ryan1.5b
        pad_token_id: 1  # tokenizer.eos_token_id
        torch_dtype: auto
        low_cpu_mem_usage: true

    checkpoint_dir: ckpt
    logging_dir: logs
    output_dir: model

TRAININGARGS:
    per_device_train_batch_size: 32
    per_device_eval_batch_size: 32
    num_train_epochs: 5
    learning_rate: 1e-5
    evaluation_strategy: steps
    eval_steps: 1000
    save_strategy: steps
    save_steps: 1000
    save_total_limit: 50
    logging_steps: 100
    report_to: wandb
    run_name: hackathon

DATASETS:
    seq_len: 256
    train_data_path: train.json
    eval_data_path: eval.json
    batch_size: 1000

ETC:
    random_seed: 42
    wandb_project: hackathon
    wandb_entity: asdf

# remove hydra logger
hydra:
    output_subdir: null
    run:
        dir: .

defaults:
    - _self_
    - override hydra/hydra_logging: disabled
    - override hydra/job_logging: disabled
